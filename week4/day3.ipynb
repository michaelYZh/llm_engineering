{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier model to generate high performance C++ code from Python code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder: fetch latest code</h2>\n",
    "            <span style=\"color:#f71;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in a Cursor Terminal, run:<br/>\n",
    "            <code>uv sync</code><br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h1 style=\"color:#900;\">Important Note</h1>\n",
    "            <span style=\"color:#900;\">\n",
    "            In this lab, I use high end models GPT 5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Grok 4, which are the slightly higher priced models. The costs are still low, but if you'd prefer to keep costs ultra low, please pick lower cost models like gpt-5-nano.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "Grok API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59863df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-5\"\n",
    "CLAUDE_MODEL = \"claude-sonnet-4-5-20250929\"\n",
    "# GROK_MODEL = \"grok-4\"\n",
    "GEMINI_MODEL = \"gemini-2.5-pro\"\n",
    "\n",
    "# Want to keep costs ultra-low? Uncomment these lines:\n",
    "\n",
    "# OPENAI_MODEL = \"gpt-5-nano\"\n",
    "# CLAUDE_MODEL = \"claude-3-5-haiku-latest\"\n",
    "# GROK_MODEL = \"grok-4-fast-non-reasoning\"\n",
    "# GEMINI_MODEL = \"gemini-2.5-flash-lite\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab38a7",
   "metadata": {},
   "source": [
    "## PLEASE NOTE:\n",
    "\n",
    "We will be writing a solution to convert Python into efficient, optimized C++ code for your machine, which can be compiled to native machine code and executed.\n",
    "\n",
    "It is not necessary for you to execute the code yourself - that's not the point of the exercise!\n",
    "\n",
    "But if you would like to (because it's satisfying!) then I'm including the steps here. Very optional!\n",
    "\n",
    "As an alternative, I'll also show you a website where you can run the C++ code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2fbb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os': {'system': 'Darwin',\n",
       "  'arch': 'arm64',\n",
       "  'release': '24.6.0',\n",
       "  'version': 'Darwin Kernel Version 24.6.0: Mon Aug 11 21:15:09 PDT 2025; root:xnu-11417.140.69.701.11~1/RELEASE_ARM64_T6041',\n",
       "  'kernel': '24.6.0',\n",
       "  'distro': None,\n",
       "  'wsl': False,\n",
       "  'rosetta2_translated': False,\n",
       "  'target_triple': 'arm64-apple-darwin24.6.0'},\n",
       " 'package_managers': ['xcode-select (CLT)', 'brew'],\n",
       " 'cpu': {'brand': 'Apple M4 Pro',\n",
       "  'cores_logical': 14,\n",
       "  'cores_physical': 14,\n",
       "  'simd': []},\n",
       " 'toolchain': {'compilers': {'gcc': 'Apple clang version 17.0.0 (clang-1700.3.19.1)',\n",
       "   'g++': 'Apple clang version 17.0.0 (clang-1700.3.19.1)',\n",
       "   'clang': 'Apple clang version 17.0.0 (clang-1700.3.19.1)',\n",
       "   'msvc_cl': ''},\n",
       "  'build_tools': {'cmake': 'cmake version 4.1.2',\n",
       "   'ninja': '',\n",
       "   'make': 'GNU Make 3.81'},\n",
       "  'linkers': {'ld_lld': ''}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from system_info import retrieve_system_info\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "system_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d29a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Excellent, you have provided very clear system information. Let's get straight to the answers.\n",
       "\n",
       "### Do you need to install a C++ compiler?\n",
       "\n",
       "No, you do not need to install a C++ compiler.\n",
       "\n",
       "Your system report clearly shows that you are already set up. Specifically, this section indicates you have the necessary tools installed via Apple's Xcode Command Line Tools:\n",
       "\n",
       "```\n",
       "'toolchain': {\n",
       "    'compilers': {\n",
       "        'g++': 'Apple clang version 17.0.0 (clang-1700.3.19.1)',\n",
       "        'clang': 'Apple clang version 17.0.0 (clang-1700.3.19.1)'\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "On macOS, `g++` is an alias for `clang++`, which is Apple's excellent, modern C++ compiler. You are ready to go.\n",
       "\n",
       "---\n",
       "\n",
       "### Python Script to Compile and Run\n",
       "\n",
       "Here is the Python code with the `compile_command` and `run_command` variables filled in to compile your `main.cpp` for the best possible runtime performance and then execute it.\n",
       "\n",
       "```python\n",
       "import subprocess\n",
       "import os\n",
       "\n",
       "# Ensure there is a main.cpp file to compile for this example to work.\n",
       "# For example, create a file named main.cpp with the following content:\n",
       "# #include <iostream>\n",
       "# int main() {\n",
       "#     std::cout << \"Hello from C++ on Apple M4 Pro!\" << std::endl;\n",
       "#     return 0;\n",
       "# }\n",
       "\n",
       "def compile_and_run_cpp():\n",
       "    \"\"\"\n",
       "    Compiles main.cpp for maximum performance and runs the resulting executable.\n",
       "    \"\"\"\n",
       "    source_file = \"main.cpp\"\n",
       "    executable_name = \"main_executable\"\n",
       "\n",
       "    # For the fastest possible runtime performance, we use high optimization levels.\n",
       "    # On macOS, g++ is an alias for clang++, so we can use either.\n",
       "    compile_command = [\n",
       "        \"clang++\",\n",
       "        \"-std=c++17\",      # Use a modern C++ standard\n",
       "        \"-O3\",             # The highest standard level of optimization for speed\n",
       "        \"-march=native\",   # Optimize specifically for your M4 Pro CPU\n",
       "        \"-DNDEBUG\",        # Disables asserts and other debug code\n",
       "        \"-o\", executable_name, # Specify the output file name\n",
       "        source_file        # The input C++ source file\n",
       "    ]\n",
       "\n",
       "    run_command = [f\"./{executable_name}\"]\n",
       "\n",
       "    try:\n",
       "        print(f\"Compiling with command: {' '.join(compile_command)}\")\n",
       "        # Compile the C++ code\n",
       "        compile_result = subprocess.run(\n",
       "            compile_command, \n",
       "            check=True,          # Raises an exception if compilation fails\n",
       "            text=True, \n",
       "            capture_output=True\n",
       "        )\n",
       "        if compile_result.stderr:\n",
       "            print(\"Compiler Warnings/Messages:\\n\", compile_result.stderr)\n",
       "        \n",
       "        print(f\"\\nRunning with command: {' '.join(run_command)}\")\n",
       "        # Run the compiled executable\n",
       "        run_result = subprocess.run(\n",
       "            run_command, \n",
       "            check=True, \n",
       "            text=True, \n",
       "            capture_output=True\n",
       "        )\n",
       "\n",
       "        # Clean up the created executable\n",
       "        os.remove(executable_name)\n",
       "\n",
       "        return run_result.stdout\n",
       "\n",
       "    except FileNotFoundError:\n",
       "        return f\"Error: Command not found. Is '{compile_command[0]}' in your PATH?\"\n",
       "    except subprocess.CalledProcessError as e:\n",
       "        # This will catch errors from both compilation and execution\n",
       "        error_message = (\n",
       "            f\"An error occurred.\\n\"\n",
       "            f\"Command: {' '.join(e.cmd)}\\n\"\n",
       "            f\"Return Code: {e.returncode}\\n\"\n",
       "            f\"Stdout:\\n{e.stdout}\\n\"\n",
       "            f\"Stderr:\\n{e.stderr}\"\n",
       "        )\n",
       "        return error_message\n",
       "\n",
       "# --- To run this code ---\n",
       "# 1. Save the python script (e.g., as `runner.py`).\n",
       "# 2. Create a file named `main.cpp` in the same directory.\n",
       "# 3. Run the python script from your terminal: `python3 runner.py`\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    output = compile_and_run_cpp()\n",
       "    print(\"\\n--- C++ Program Output ---\")\n",
       "    print(output)\n",
       "    print(\"--------------------------\")\n",
       "```\n",
       "\n",
       "### Explanation of the Commands\n",
       "\n",
       "#### `compile_command`\n",
       "`['clang++', '-std=c++17', '-O3', '-march=native', '-DNDEBUG', '-o', 'main_executable', 'main.cpp']`\n",
       "\n",
       "*   `clang++`: The C++ compiler on your system.\n",
       "*   `-std=c++17`: Specifies that the code should be compiled using the C++17 standard. This is a good, modern default. You could also use `-std=c++20`.\n",
       "*   `-O3`: This is the key flag for performance. It tells the compiler to apply a high level of aggressive optimizations to make the code run as fast as possible.\n",
       "*   `-march=native`: This is another important performance flag. It instructs the compiler to generate code specifically optimized for the CPU you are compiling on (in your case, the Apple M4 Pro). This can unlock specific instruction sets for better performance.\n",
       "*   `-DNDEBUG`: This preprocessor directive is standard practice for \"release\" or \"production\" builds. It typically disables `assert()` statements and other debugging-only code, which can slightly improve performance and reduce binary size.\n",
       "*   `-o main_executable`: This sets the name of the output file (the compiled program) to `main_executable`. If you omit this, it will default to `a.out`.\n",
       "*   `main.cpp`: This is your input source file.\n",
       "\n",
       "#### `run_command`\n",
       "`['./main_executable']`\n",
       "\n",
       "*   `./main_executable`: This command tells the shell to execute the file named `main_executable` located in the current directory (`.`)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a C++ compiler to compile a single C++ file called main.cpp and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install any C++ compiler to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile C++ code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "```python\n",
    "compile_command = # something here - to achieve the fastest possible runtime performance\n",
    "compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "run_command = # something here\n",
    "run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "return run_result.stdout\n",
    "```\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "\n",
    "System information:\n",
    "{system_info}\n",
    "\"\"\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=GEMINI_MODEL, messages=[{\"role\": \"user\", \"content\": message}])\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e92c12",
   "metadata": {},
   "source": [
    "## If you need to install something\n",
    "\n",
    "If you would like to, please follow GPTs instructions! Then rerun the analysis afterwards (you might need to Restart the notebook) to confirm you're set.\n",
    "\n",
    "You should now be equipped with the command to compile the code, and the command to run it!\n",
    "\n",
    "Enter that in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d734a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_command = [\"clang++\", \"-std=c++17\", \"-Ofast\", \"-mcpu=native\", \"-flto=thin\", \"-fvisibility=hidden\", \"-DNDEBUG\", \"main.cpp\", \"-o\", \"main\"]\n",
    "run_command = [\"./main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0a437",
   "metadata": {},
   "source": [
    "## And now, on with the main task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your task is to convert Python code into high performance C++ code.\n",
    "Respond only with C++ code. Do not provide any explanation other than occasional comments.\n",
    "The C++ response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to C++ with the fastest possible implementation that produces identical output in the least time.\n",
    "The system information is:\n",
    "{system_info}\n",
    "Your response will be written to a file called main.cpp and then compiled and executed; the compilation command is:\n",
    "{compile_command}\n",
    "Respond only with C++ code.\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(cpp):\n",
    "    with open(\"main.cpp\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(client, model, python):\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```','')\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1cbb778-fa57-43de-b04b-ed523f396c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(400_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    "    globals = {\"__builtins__\": __builtins__}\n",
    "    exec(code, globals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7faa90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592654838\n",
      "Execution Time: 21.911236 seconds\n"
     ]
    }
   ],
   "source": [
    "run_python(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "105db6f9-343c-491d-8e44-3a5328b81719",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(anthropic, CLAUDE_MODEL, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f8018-f64d-425c-a0e1-d7862aa9592d",
   "metadata": {},
   "source": [
    "# Compiling C++ and executing\n",
    "\n",
    "This next cell contains the command to compile a C++ file based on the instructions from GPT.\n",
    "\n",
    "Again, it's not crucial to do this step if you don't wish to!\n",
    "\n",
    "OR alternatively: student Sandeep K.G. points out that you can run Python and C++ code online to test it out that way. Thank you Sandeep!  \n",
    "> Not an exact comparison but you can still get the idea of performance difference.  \n",
    "> For example here: https://www.programiz.com/cpp-programming/online-compiler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the commands from GPT 5\n",
    "\n",
    "def compile_and_run():\n",
    "    subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
    "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
    "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22f8f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592654839\n",
      "Execution Time: 0.162977 seconds\n",
      "\n",
      "Result: 3.141592654839\n",
      "Execution Time: 0.150832 seconds\n",
      "\n",
      "Result: 3.141592654839\n",
      "Execution Time: 0.153744 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compile_and_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faaa39de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.44373132405184"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21.911236/0.162977"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b8ef9",
   "metadata": {},
   "source": [
    "## OK let's try the other contenders!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a11fe-e24d-4c65-8269-9802c5ef3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(anthropic, CLAUDE_MODEL, pi)\n",
    "compile_and_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(grok, GROK_MODEL, pi)\n",
    "compile_and_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a0243c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592654838\n",
      "Execution Time: 0.025465 seconds\n",
      "\n",
      "Result: 3.141592654838\n",
      "Execution Time: 0.022107 seconds\n",
      "\n",
      "Result: 3.141592654838\n",
      "Execution Time: 0.022870 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port(gemini, GEMINI_MODEL, pi)\n",
    "compile_and_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689e200",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "In Ed's experiments, the performance speedups were:\n",
    "\n",
    "4th place: Claude Sonnet 4.5: {19.178207/0.104241:.0f}X speedup\n",
    "3rd place: GPT-5: {19.178207/0.082168:.0f}X speedup\n",
    "2nd place: Grok 4: {19.178207/0.018092:.0f}X speedup\n",
    "1st place: Gemini 2.5 Pro: {19.178207/0.013314:.0f}X speedup\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
